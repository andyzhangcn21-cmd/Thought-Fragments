\begin{thebibliography}{10}

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock {\em Deep Learning}.
\newblock MIT Press, 2016.

\bibitem{amari2000}
S.~Amari and H.~Nagaoka.
\newblock {\em Methods of Information Geometry}.
\newblock Oxford University Press, 2000.

\bibitem{amari1998}
S.~Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276, 1998.

\bibitem{fefferman2016testing}
C.~Fefferman, S.~Mitter, and H.~Narayanan.
\newblock Testing the manifold hypothesis.
\newblock {\em Journal of the American Mathematical Society}, 29(4):983--1049,
  2016.

\bibitem{bronstein2021}
M.~M. Bronstein et~al.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges.
\newblock {\em arXiv preprint arXiv:2104.13478}, 2021.

\bibitem{arvanitidis2018geometrically}
G.~Arvanitidis, L.~K. Hansen, and S.~Hauberg.
\newblock Geometrically enriched latent spaces.
\newblock {\em arXiv preprint arXiv:1805.07666}, 2018.

\bibitem{rezende2015}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock {\em International Conference on Machine Learning}, pages 1530--1538,
  2015.

\bibitem{kobyzev2020}
I.~Kobyzev, S.~J. Prince, and M.~A. Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  43(11):3964--3979, 2020.

\bibitem{papamakarios2021normalizing}
G.~Papamakarios, E.~Nalisnick, D.~J. Rezende, S.~Mohamed, and
  B.~Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em Journal of Machine Learning Research}, 22(57):1--64, 2021.

\bibitem{belkin2003laplacian}
M.~Belkin and P.~Niyogi.
\newblock Laplacian eigenmaps for dimensionality reduction and data
  representation.
\newblock {\em Neural computation}, 15(6):1373--1396, 2003.

\bibitem{dinh2016}
L.~Dinh, J.~Sohl-Dickstein, and S.~Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{kingma2018}
D.~P. Kingma and P.~Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem{liu2020understanding}
L.~Liu, H.~Jiang, P.~He, W.~Chen, X.~Liu, J.~Gao, and J.~Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em arXiv preprint arXiv:1908.03265}, 2020.

\bibitem{cohen2021gradient}
J.~Cohen, S.~Kaur, Y.~Li, J.~Z. Kolter, and A.~Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock {\em arXiv preprint arXiv:2103.00065}, 2021.

\bibitem{martens2015}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock {\em International conference on machine learning}, pages 2408--2417,
  2015.

\bibitem{stringer2019}
C.~Stringer et~al.
\newblock High-dimensional geometry of population responses in visual cortex.
\newblock {\em Nature}, 571(7765):361--365, 2019.

\bibitem{wasserman2018topological}
L.~Wasserman.
\newblock Topological data analysis.
\newblock {\em Annual Review of Statistics and Its Application}, 5:501--532,
  2018.

\end{thebibliography}
